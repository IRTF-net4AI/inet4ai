description: >
  Generative AI systems are approaching a scalability limit in their development. 
  Due to power density issues, it will soon become infeasible to train large language 
  models with an increasing number of parameters in a single datacenter. 
  While the industry is actively pursuing an effort to scale up AI systems, 
  it becomes necessary to explore the use of scaled-out, global distributed systems 
  to train or serve generative AI models. 
  
  Besides, services based on generative AI ask for stringent quality of service 
  levels to meet users demand. Meeting those requirements can be addressed by 
  using systems mixing powerful computing instances residing in cloud platforms 
  with localized edge platforms, using heterogeneous and distributed systems.
  
  Those questions may find a solution in approaches adopted by  federated learning 
  systems, in which models are trained among several stakeholders. Yet, those 
  systems also face scalability issues in dealing with models of a larger size. 
  
  This workshop, initiated in the realm of the Internet Research Task Force (IRTF), 
  aims at discussing the networking challenges raised by the distribution of generative 
  AI workloads at a large scale. To that extend, we aim at receiving contributions 
  from academic researchers, machine learning system developers or AI infrastructure 
  providers on the following topics:

submission: ./submission.html
areas:
- category: "Sanskrit Computational Linguistics: with Extensions to Related Ancient Indian Languages (Vedic, Pali, Prakrit)"
  topics:
    - Digital lexicons, thesauri and wordnets
    - Computational phonology and morphology
    - Syntactic analysis
    - Prose order normalisation
    - Parsing
    - Discourse analysis
    - Structural semantics
    - Machine translation
    - Automatic analysis of Sanskrit corpus
    - Navya-NyƒÅya and other technical language processing
    - Information extraction
    - Named entity recognition
